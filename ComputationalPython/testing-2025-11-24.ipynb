{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "NAME = \"David Marcell Prifer\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92a3e7f423c94217e866c1d4bb0a3d23",
     "grade": false,
     "grade_id": "cell-c374456facd20e46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Testing\n",
    "\n",
    "Testing is one of the most important components of sustainable software development. It improves code quality, maintainability and lifetime, and saves developer time. In previous labs you have already come across this in the form of assert statements. There are a number of testing libraries to support Python development and they all have in common that they build on the assert statement. In addition they provide better information when errors are found and the facilitate automated testing of large projects.\n",
    "\n",
    "We will use the doctest and the pytest libraries. If pytest is not installed you need to install it with pip. On your computers do this in the virtual environment where you previously installed jupyter\n",
    "\n",
    "~~~\n",
    "$ conda install pytest\n",
    "~~~\n",
    "\n",
    "Testing libraries are typically designed to be used on Python source files while they are not adapted to be used with Jupyter notebooks. To be able to work with this in this lab, we use cell magic commands (%%file) to save cells in ordinary files and then execute pytest on those files\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b419f62e162c6a542eeacf8bdb910bba",
     "grade": false,
     "grade_id": "cell-33440c27e09b63e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-11-24T12:47:28.788164Z",
     "start_time": "2025-11-24T12:47:28.587089Z"
    }
   },
   "source": [
    "import doctest\n",
    "import pytest\n",
    "# This is for jupyter to recognize changes in external files without restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: add time stamps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2655a28b70be86fdb6d905e6ca8e95b",
     "grade": false,
     "grade_id": "cell-01f525753e8b1889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-11-24T12:47:46.898969Z",
     "start_time": "2025-11-24T12:47:46.810104Z"
    }
   },
   "source": [
    "%%file timestamps.py\n",
    "from timestamps1 import timestamp_to_seconds\n",
    "from timestamps2 import seconds_to_timestamp\n",
    "\n",
    "def sum_timestamps(l):\n",
    "    \"\"\"\n",
    "    >>> sum_timestamps(['5:32', '4:48'])\n",
    "    '10:20'\n",
    "    >>> sum_timestamps(['03:10', '01:00'])\n",
    "    '4:10'\n",
    "    >>> sum_timestamps(['2:10', '1:59'])\n",
    "    '4:09'\n",
    "    >>> sum_timestamps(['15:32', '45:48'])\n",
    "    '1:01:20'\n",
    "    >>> sum_timestamps(['6:15:32', '2:45:48'])\n",
    "    '9:01:20'\n",
    "    >>> sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
    "    '10:01:30'\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for ts in l:    \n",
    "        total += timestamp_to_seconds(ts)\n",
    "        \n",
    "    total_as_timestamp = seconds_to_timestamp(total)\n",
    "    return total_as_timestamp\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing timestamps.py\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6ab2fbf16c104e844b6a490bbd1f663",
     "grade": false,
     "grade_id": "cell-0ea01ce4cc34e8ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-11-24T12:48:51.226520Z",
     "start_time": "2025-11-24T12:48:51.191685Z"
    }
   },
   "source": [
    "%%file timestamps1.py\n",
    "def timestamp_to_seconds(ts):\n",
    "    \"\"\"\n",
    "    >>> timestamp_to_seconds(\"1:01\")\n",
    "    61\n",
    "    >>> timestamp_to_seconds(\"1:00:00\")\n",
    "    3600\n",
    "    \"\"\"\n",
    "    fields = ts.split(':')\n",
    "    if len(fields) == 2:\n",
    "        minutes, seconds = fields\n",
    "        total = 60*int(minutes) + int(seconds)\n",
    "        return total\n",
    "    elif len(fields) == 3:\n",
    "        hours, minutes, seconds = fields\n",
    "        total = 3600*int(hours) + 60*int(minutes) + int(seconds)\n",
    "        return total"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing timestamps1.py\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b894b419585c397b266e9d2c3c27f3a2",
     "grade": true,
     "grade_id": "cell-1b37e8d4ce5a07e1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-11-24T12:50:03.736745Z",
     "start_time": "2025-11-24T12:50:03.690825Z"
    }
   },
   "source": [
    "import timestamps1\n",
    "doctest.testmod(timestamps1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "521ca8309e82b59d28ad512172575cbf",
     "grade": false,
     "grade_id": "cell-79d2018d1ab06c44",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-11-24T13:00:03.115636Z",
     "start_time": "2025-11-24T13:00:03.085241Z"
    }
   },
   "source": [
    "%%file timestamps2.py\n",
    "def seconds_to_timestamp(seconds):\n",
    "    \"\"\"\n",
    "    >>> seconds_to_timestamp(61)\n",
    "    '1:01'\n",
    "    >>> seconds_to_timestamp(3600)\n",
    "    '1:00:00'\n",
    "    \"\"\"\n",
    "    if seconds >= 3600:\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        sec = seconds % 60\n",
    "        return  f'{hours:1}:{minutes:02}:{sec:02}'\n",
    "    elif seconds >= 60:\n",
    "        minutes = seconds // 60\n",
    "        sec = seconds % 60\n",
    "        return f'{minutes:1}:{sec:02}'\n",
    "    else:\n",
    "        return seconds_to_timestamp(seconds)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing timestamps2.py\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b3f9f326997c0ac4a76e69a6df58b4a",
     "grade": true,
     "grade_id": "cell-210600fd395cba29",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-11-24T13:00:09.164817Z",
     "start_time": "2025-11-24T13:00:09.113613Z"
    }
   },
   "source": [
    "import timestamps2\n",
    "from timestamps2 import *\n",
    "doctest.testmod(timestamps2, verbose=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: time_stamps with pytest\n",
    "\n",
    "Write a test file to be used with pytest for assignment 1, with the same test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd9da0bea9a1e08f2cbff299658b314b",
     "grade": false,
     "grade_id": "cell-ed7cd15078053e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%file test_timestamps.py\n",
    "import timestamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee29c06d79b8bcedd807baa0cfac3473",
     "grade": true,
     "grade_id": "cell-c597b267d405be1f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pytest.main(['-v', 'test_timestamps.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4a31a576f7a13a7f174e005cf620b",
     "grade": false,
     "grade_id": "cell-a4c4bdbc6f98dac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: running mean\n",
    "\n",
    "This is an example use of parametrized test cases. Look at the test function and understand how it works.\n",
    "Write a function that passes the tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15c3812d0a4c64c65dfe19947283f0b",
     "grade": false,
     "grade_id": "cell-a62f5af5275891d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%file test_running_mean.py\n",
    "import pytest\n",
    "\n",
    "from running_mean import running_mean\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    ([1, 2, 3], [1, 1.5, 2]),\n",
    "    ([2, 6, 10, 8, 11, 10],\n",
    "     [2.0, 4.0, 6.0, 6.5, 7.4, 7.83]),\n",
    "    ([3, 4, 6, 2, 1, 9, 0, 7, 5, 8],\n",
    "     [3.0, 3.5, 4.33, 3.75, 3.2, 4.17, 3.57, 4.0, 4.11, 4.5]),\n",
    "    ([], []),\n",
    "])\n",
    "def test_running_mean(input_argument, expected_return):\n",
    "    ret = list(running_mean(input_argument))\n",
    "    assert ret == expected_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "044e69ec2f80299d5a2958b662233328",
     "grade": false,
     "grade_id": "cell-96c10ee58ecac500",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%file running_mean.py\n",
    "def running_mean(sequence):\n",
    "    \"\"\"Calculate the running mean of the sequence passed in,\n",
    "       returns a sequence of same length with the averages.\n",
    "       You can assume all items in sequence are numeric.\"\"\"\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2214c633a7a294cb33de7bbac7d2bad5",
     "grade": true,
     "grade_id": "cell-6a057ac659530fd9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pytest.main(['-v', 'test_running_mean.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "Write a second version of the test function for timestamp where the different test cases are different parameterizations for one test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c793f47e4a9c57a14319595e0e18bdf",
     "grade": false,
     "grade_id": "cell-1d068aaf56835af8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%file test_timestamps_parametrized.py\n",
    "import pytest\n",
    "from timestamps import sum_timestamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8b2d982b43e73f1cb7e07d5806071ea",
     "grade": true,
     "grade_id": "cell-32b906546d801ca5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pytest.main(['-v', 'test_timestamps_parametrized.py'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
